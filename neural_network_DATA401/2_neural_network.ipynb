{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. NeuralNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    y = x > 0\n",
    "    return y.astype(int)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x)-np.exp(-1*x))/(np.exp(x)+np.exp(-1*x))\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - (tanh(x)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize each column\n",
    "df = pd.read_csv('data.csv',index_col=0)\n",
    "for col in df:\n",
    "    df[col] = (df[col] - df[col].min())/(df[col].max() - df[col].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['num_room', 'office_count_5000', 'culture_objects_top_25_raion',\n",
    "           'ekder_all', 'floor', 'bandwidth_sports', 'park_km',\n",
    "           'big_church_count_5000', 'indust_part', 'healthcare_centers_raion',\n",
    "           'mosque_count_5000', 'ecology', 'big_road1_km', 'fitness_km',\n",
    "           'big_market_raion', 'product_type', 'railroad_terminal_raion',\n",
    "           'university_top_20_raion', 'nuclear_reactor_raion', 'cpi',\n",
    "           'green_zone_part', 'work_all', 'children_school', 'radiation_raion']]\n",
    "y = df[['price_doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    \"\"\"\n",
    "    A class representing a fully-connected, feed-forward neural network.\n",
    "    \n",
    "    Params:\n",
    "        nlayers: the total number of hidden layers in the network\n",
    "        nnodes: an array containing the number of nodes in each layer\n",
    "        activations: an array containing the names of activation functions to use in each layer\n",
    "            (note that activations[0] and activations[nlayers+2] will not be used)\n",
    "            \n",
    "    Notes:\n",
    "        All hidden layers and the input layer also include an intercept node, not counted\n",
    "        in nnodes. Intercept nodes are not connected to any nodes in the previous layer\n",
    "        and have a value of 1; they are connected to all nodes in the next layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nlayers, nnodes, activations):\n",
    "        assert nlayers == len(nnodes) - 2\n",
    "        assert nlayers == len(activations) - 2\n",
    "        self.nlayers = nlayers\n",
    "        self.nnodes = nnodes\n",
    "        self.activations = activations\n",
    "        self.weights = self.initialize_weights()\n",
    "        self.z = []\n",
    "        self.h = []\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        \"\"\" \n",
    "        Randomly initialize all weights to numbers in [-0.25,0.25].\n",
    "        \n",
    "        Returns:\n",
    "            An array of length nlayers+1 where element i is an array of length \n",
    "            (nnodes[i]+1)*nnodes[i+1] containing random weights for each edge between\n",
    "            layers i and i+1, including weights corresponding to the intercept node.\n",
    "        \"\"\"\n",
    "        weights = []\n",
    "        for i in range(self.nlayers+1):\n",
    "            weights_i = []\n",
    "            for j in range((self.nnodes[i]+1)*self.nnodes[i+1]):\n",
    "                weights_i.append(np.random.uniform(-0.25,0.25))\n",
    "            weights.append(weights_i)\n",
    "        return weights\n",
    "    \n",
    "    def chunker(self, seq, size):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "    \n",
    "    def forward_prop(self, input_data):\n",
    "        \"\"\" \n",
    "        Propagates input matrix x through the neural network, saving intermediate values\n",
    "        of z and h. Adds intercept node to each layer.\n",
    "        \n",
    "        Params:\n",
    "            inputs: an array of arrays of input values\n",
    "        \n",
    "        Returns:\n",
    "            an array of output values\n",
    "        \"\"\"\n",
    "        assert len(input_data[0]) == self.nnodes[0]\n",
    "        self.z=[]\n",
    "        inputs=[0]\n",
    "        z=[]\n",
    "        inputs[0] = np.repeat(1,len(input_data))\n",
    "        for i in range(len(input_data[0])):\n",
    "            add = np.array([j[i] for j in input_data])\n",
    "            inputs.append(add)\n",
    "            z.append(add)\n",
    "        self.z.append(z)\n",
    "        self.h = self.z.copy()\n",
    "        for i in range(self.nlayers+1):\n",
    "            new_nodes = [0] * self.nnodes[i+1]\n",
    "            p = 0\n",
    "            #go through the weights for each input node one group at a time \n",
    "            for w in self.chunker(self.weights[i], len(new_nodes)):\n",
    "                for j in range(len(w)):\n",
    "                    # add the value corresponding to the jth node in the next layer\n",
    "                    new_nodes[j]= new_nodes[j]+ w[j]*inputs[p]\n",
    "                p += 1\n",
    "            self.z.append(new_nodes)\n",
    "            self.h.append([self.activations[i+1](j) for j in new_nodes])\n",
    "            inputs = self.h[i+1].copy()\n",
    "            inputs.insert(0,np.repeat(1,len(inputs[0])))\n",
    "        return self.z[self.nlayers+1]\n",
    "    \n",
    "    def back_prop(self, y_pred, y, rate, derivs):\n",
    "        \"\"\" \n",
    "        Performs one iteration of backpropagation.\n",
    "        \n",
    "        Params:\n",
    "            y_pred: the predicted y value\n",
    "            y: the true y value\n",
    "            rate: the learning rate\n",
    "            derivs: array of the derivatives of each activation function\n",
    "                (note again that derivs[0] and derivs[nlayers+2] will not be used))\n",
    "        \n",
    "        Returns:\n",
    "            an array of updated weights\n",
    "        \"\"\"\n",
    "        deltas = y_pred - y[0]\n",
    "        new_weights = []\n",
    "        for layer in range(self.nlayers,-1,-1):\n",
    "            i=0\n",
    "            new_w = []\n",
    "            n = len(self.h[layer][0])\n",
    "            h_vals = self.h[layer].copy()\n",
    "            h_vals.insert(0,np.repeat(1,n))\n",
    "            for h in h_vals:\n",
    "                for d in deltas:\n",
    "                    #sum weight changes across observations\n",
    "                    changes = np.sum(h*d)\n",
    "                    old_w = self.weights[layer][i]\n",
    "                    new_w.append(old_w - rate*changes)\n",
    "                    i+=1\n",
    "            new_weights.insert(0,new_w)\n",
    "            i=len(self.z[layer+1])\n",
    "            new_deltas = []\n",
    "            for z in self.z[layer]:\n",
    "                new_d=0\n",
    "                for d in deltas:\n",
    "                    new_d += derivs[layer](z)*d*self.weights[layer][i]\n",
    "                    i+=1\n",
    "                new_deltas.append(new_d)\n",
    "            deltas = new_deltas\n",
    "        self.weights = new_weights\n",
    "                \n",
    "    def gradient_descent(self, data, y_val, rate, batch_size, derivs, tol):\n",
    "        \"\"\" \n",
    "        Performs stochastic gradient descent to train the weights of the neural network.\n",
    "        \n",
    "        Params:\n",
    "            data: the full dataset, not including the target variable\n",
    "            y_val: the target column\n",
    "            rate: the learning rate\n",
    "            batch_size: the number of observations in each batch\n",
    "            derivs: array of the derivatives of each activation function\n",
    "                (note again that derivs[0] and derivs[nlayers+2] will not be used))\n",
    "            tol: tolerance (difference in MSEs to stop at)\n",
    "        \n",
    "        Returns:\n",
    "            an array of weights of optimal neural network\n",
    "        \"\"\"\n",
    "        diff = 100000\n",
    "        new_MSE = 0\n",
    "        while diff > tol:\n",
    "            old_MSE = new_MSE\n",
    "            new_MSE = 0\n",
    "            prev = 0\n",
    "            while prev < len(data):\n",
    "                nxt = prev+batch_size\n",
    "                if nxt > len(data):\n",
    "                    nxt = len(data)\n",
    "                xs = data.values[prev:nxt]\n",
    "                ys = y_val.values[prev:nxt]\n",
    "                prev = nxt\n",
    "                y_pred = self.forward_prop(xs)\n",
    "                self.back_prop(y_pred,ys,rate,derivs)\n",
    "                new_MSE += np.sum((self.forward_prop(xs)[0]-ys[0])**2)\n",
    "            new_MSE = new_MSE/len(data)\n",
    "            diff = abs(old_MSE - new_MSE)\n",
    "            print(\"MSE = \"+str(new_MSE))\n",
    "        return new_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploration of nnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 layer | 30 nodes | <function tanh at 0x7f12ea42cf28>\n",
      "\t MSE = 0.00015857060495233754\n",
      "1 layer | 30 nodes | <function sigmoid at 0x7f12ea1e4c80>\n",
      "\t MSE = 0.00014449458373616648\n",
      "1 layer | 30 nodes | <function relu at 0x7f12ea17cf28>\n",
      "\t MSE = 0.0012775878677628895\n",
      "1 layer | 25 nodes | <function tanh at 0x7f12ea42cf28>\n",
      "\t MSE = 0.0001294302115504122\n",
      "1 layer | 25 nodes | <function sigmoid at 0x7f12ea1e4c80>\n",
      "\t MSE = 9.050155408528709e-05\n",
      "1 layer | 25 nodes | <function relu at 0x7f12ea17cf28>\n",
      "\t MSE = 0.0014611746033805063\n",
      "1 layer | 20 nodes | <function tanh at 0x7f12ea42cf28>\n",
      "\t MSE = 0.00017197283771894212\n",
      "1 layer | 20 nodes | <function sigmoid at 0x7f12ea1e4c80>\n",
      "\t MSE = 8.623543771199027e-05\n",
      "1 layer | 20 nodes | <function relu at 0x7f12ea17cf28>\n",
      "\t MSE = 0.0014883837122360473\n",
      "1 layer | 15 nodes | <function tanh at 0x7f12ea42cf28>\n",
      "\t MSE = 0.000216297387367162\n",
      "1 layer | 15 nodes | <function sigmoid at 0x7f12ea1e4c80>\n",
      "\t MSE = 0.00047199819345060325\n",
      "1 layer | 15 nodes | <function relu at 0x7f12ea17cf28>\n",
      "\t MSE = 0.0009592494616901898\n",
      "1 layer | 10 nodes | <function tanh at 0x7f12ea42cf28>\n",
      "\t MSE = 0.0005777215397317788\n",
      "1 layer | 10 nodes | <function sigmoid at 0x7f12ea1e4c80>\n",
      "\t MSE = 0.0011802664247073575\n",
      "1 layer | 10 nodes | <function relu at 0x7f12ea17cf28>\n",
      "\t MSE = 0.0002781401011747028\n",
      "1 layer | 5 nodes | <function tanh at 0x7f12ea42cf28>\n",
      "\t MSE = 0.0010480717657489265\n",
      "1 layer | 5 nodes | <function sigmoid at 0x7f12ea1e4c80>\n",
      "\t MSE = 0.0012482999565369265\n",
      "1 layer | 5 nodes | <function relu at 0x7f12ea17cf28>\n",
      "\t MSE = 0.0015853352425459885\n"
     ]
    }
   ],
   "source": [
    "nodes = [30,25,20,15,10,5]\n",
    "act = [tanh, sigmoid, relu]\n",
    "der = [tanh_derivative, sigmoid_derivative, relu_derivative]\n",
    "\n",
    "for node in nodes:\n",
    "    for (f,d) in zip(act,der):\n",
    "        nnodes = [24]\n",
    "        activations = [relu]\n",
    "        derivs = [relu_derivative]\n",
    "        nnodes.append(node)\n",
    "        activations.append(f)\n",
    "        derivs.append(d)\n",
    "        nnodes.append(1)\n",
    "        activations.append(relu)\n",
    "        derivs.append(relu_derivative)\n",
    "        print(\"1 layer | \" + str(node) + \" nodes | \" + str(f))\n",
    "        nn = NeuralNetwork(nlayers=1, nnodes=nnodes, activations=activations)\n",
    "        print(\"\\t MSE = \" + str(nn.gradient_descent(data,y,0.001,100,derivs,0.000001)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random & Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "import sys\n",
    "import random\n",
    "\n",
    "def random_search(X, y, num_models=50, \n",
    "                  layer_options=[1,2,3,4,5], \n",
    "                  nnodes_options=[5, 8, 10, 15, 20, 25, 30, 40]):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0,test_size=0.2)\n",
    "    results = []\n",
    "    act_options = [(tanh,tanh_derivative), (sigmoid,sigmoid_derivative), (relu,relu_derivative)]\n",
    "    best_mse = sys.maxsize\n",
    "    best_model = None\n",
    "    for model in range(num_models):\n",
    "        nlayers = random.choice(layer_options)\n",
    "        nnodes = [24]\n",
    "        activations = [relu]\n",
    "        derivs = [relu_derivative]\n",
    "        for layer in range(nlayers):\n",
    "            nnodes.append(random.choice(nnodes_options))\n",
    "            activ = random.choice(act_options)\n",
    "            activations.append(activ[0])\n",
    "            derivs.append(activ[1])\n",
    "        nnodes.append(1)\n",
    "        activations.append(relu)\n",
    "        derivs.append(relu_derivative)\n",
    "        nn = NeuralNetwork(nlayers=nlayers, nnodes=nnodes, activations=activations)\n",
    "        nn.gradient_descent(X_train,y_train,0.001,100,derivs,0.000001)\n",
    "        y_pred = nn.forward_prop(X_test.values)[0]\n",
    "        mse = mean_squared_error(y_test.values, y_pred)\n",
    "        r2 = r2_score(y_test.values, y_pred)\n",
    "        result = formulate_random_result(nlayers, nnodes, activations, derivs, mse, r2)\n",
    "        results.append(result)\n",
    "        if mse < best_mse:\n",
    "            best_model = result\n",
    "            best_mse = mse\n",
    "        del nn\n",
    "    print(\"selected best model: \"+str(best_model))\n",
    "    return results, generate_model(best_model)\n",
    "\n",
    "def grid_search(X, y, model, learning_rates=[.00001, .0001, .001], batch_sizes=[10, 30, 50, 100, 1000]):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=1)\n",
    "    results = []\n",
    "    best_mse = sys.maxsize\n",
    "    best_params = None\n",
    "    nlayers, nnodes, activations = model\n",
    "    for learning_rate in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            nn = NeuralNetwork(nlayers=nlayers, nnodes=nnodes, activations=activations)\n",
    "            nn.gradient_descent(X_train,y_train,learning_rate,batch_size,derivs,0.00001)\n",
    "            y_pred = nn.forward_prop(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            result = formulate_grid_result(nlayers, nnodes, activations, mse, r2)\n",
    "            results.append(result)\n",
    "            if mse < best_mse:\n",
    "                best_params = result\n",
    "                best_mse = mse\n",
    "            del nn\n",
    "    print(\"selected best hyperparameters: \" + str(best_params))\n",
    "    return results, generate_params(best_params)\n",
    "\n",
    "def cross_validate_new(X, y, model, folds, params):\n",
    "        sp = len(X)/folds\n",
    "        err = []\n",
    "        nlayers, nnodes, activations = model\n",
    "        learning_rate, batch_size = params\n",
    "        for i in range(folds):\n",
    "            lower = int(sp * i)\n",
    "            upper = int(sp * (i+1))\n",
    "            test_x = X.iloc[lower:upper]\n",
    "            test_y = y.iloc[lower:upper]\n",
    "            train_x = pd.concat((X.iloc[:lower], X.iloc[upper:]))\n",
    "            train_y = pd.concat((y.iloc[:lower], y.iloc[upper:]))    \n",
    "            nn = NeuralNetwork(nlayers=nlayers, nnodes=nnodes, activations=activations)\n",
    "            nn.gradient_descent(X_train,y_train,learning_rate,batch_size,derivs,0.0000001)\n",
    "            y_pred = nn.forward_prop(X_test)\n",
    "            mse = mean_squared_error(y_test.values, y_pred)\n",
    "            err.append(mse)\n",
    "        cur_err = sum(err)/len(err)\n",
    "        print(\"average MSE: \" + str(cur_err))\n",
    "\n",
    "def cv_pipeline(X, y):\n",
    "    print(\"running random search... \")\n",
    "    model = random_search(X, y)[1]\n",
    "    print(\"running grid search...\")\n",
    "    params = grid_search(X, y, model)[1]\n",
    "    nlayers, nnodes, activations = model\n",
    "    learning_rate, batch_size = params\n",
    "    scores = cross_validate_new(X, y, model, 5, params)\n",
    "    return scores\n",
    "\n",
    "def generate_model(model_dict):\n",
    "    nlayers = model_dict[\"nlayers\"]\n",
    "    nnodes = model_dict[\"nnodes\"]\n",
    "    activations = model_dict[\"activations\"]\n",
    "    return nlayers, nnodes, activations\n",
    "\n",
    "def generate_params(param_dict):\n",
    "    learning_rate = param_dict[\"nlayers\"]\n",
    "    batch_size = param_dict[\"nnodes\"]\n",
    "    return learning_rate, batch_size\n",
    "\n",
    "\n",
    "def formulate_random_result(nlayers, nnodes, activations, derivs, mse, r2):\n",
    "    ret = {}\n",
    "    model = {}\n",
    "    model[\"nlayers\"] = nlayers\n",
    "    model[\"nnodes\"] = nnodes\n",
    "    model[\"activations\"] = activations\n",
    "    model[\"derivs\"] = derivs\n",
    "    ret[\"model\"] = model\n",
    "    ret[\"mse\"] = mse\n",
    "    ret[\"r2\"] = r2\n",
    "    return ret\n",
    "\n",
    "def formulate_grid_result(learning_rate, batch_size, mse, r2):\n",
    "    ret = {}\n",
    "    params = {}\n",
    "    params[\"learning_rate\"] = learning_rate\n",
    "    params[\"batch_size\"] = batch_size\n",
    "    ret[\"params\"] = params\n",
    "    ret[\"mse\"] = mse\n",
    "    ret[\"r2\"] = r2\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'nlayers': 2, 'nnodes': [24, 15, 40, 1], 'activations': [<function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.002019139509255252, 'r2': -0.1251376385187679}\n",
      "{'model': {'nlayers': 3, 'nnodes': [24, 30, 15, 20, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function tanh at 0x7f321d42a048>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function tanh_derivative at 0x7f321d42a0d0>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0021092775500712447, 'r2': -0.17536581835455411}\n",
      "{'model': {'nlayers': 2, 'nnodes': [24, 40, 40, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019753408984861847, 'r2': -0.10073146684750567}\n",
      "{'model': {'nlayers': 2, 'nnodes': [24, 8, 20, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.002028694277598685, 'r2': -0.1304619013749233}\n",
      "{'model': {'nlayers': 4, 'nnodes': [24, 25, 8, 20, 5, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.001976002629772921, 'r2': -0.10110020747878945}\n",
      "{'model': {'nlayers': 4, 'nnodes': [24, 8, 15, 20, 20, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0021135488191773026, 'r2': -0.17774592414389634}\n",
      "{'model': {'nlayers': 3, 'nnodes': [24, 20, 20, 5, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.001967257912284423, 'r2': -0.09622733428730235}\n",
      "{'model': {'nlayers': 5, 'nnodes': [24, 15, 25, 8, 20, 15, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function sigmoid at 0x7f321d41fea0>, <function sigmoid at 0x7f321d41fea0>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function sigmoid_derivative at 0x7f321d41ff28>, <function sigmoid_derivative at 0x7f321d41ff28>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019713425492504646, 'r2': -0.09850344189111704}\n",
      "{'model': {'nlayers': 1, 'nnodes': [24, 40, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.002648769070576105, 'r2': -0.47599002614177466}\n",
      "{'model': {'nlayers': 2, 'nnodes': [24, 40, 15, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019100035064600976, 'r2': -0.06432310643742123}\n",
      "{'model': {'nlayers': 4, 'nnodes': [24, 8, 5, 40, 30, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.001965247447328768, 'r2': -0.09510703042411617}\n",
      "{'model': {'nlayers': 3, 'nnodes': [24, 10, 25, 20, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.00196598030012193, 'r2': -0.09551540253377011}\n",
      "{'model': {'nlayers': 3, 'nnodes': [24, 15, 20, 20, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0018352716649551752, 'r2': -0.022679818646959893}\n",
      "{'model': {'nlayers': 5, 'nnodes': [24, 15, 25, 30, 8, 10, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function tanh at 0x7f321d42a048>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function tanh_derivative at 0x7f321d42a0d0>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019726256091163815, 'r2': -0.09921840930223391}\n",
      "{'model': {'nlayers': 4, 'nnodes': [24, 15, 20, 25, 5, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0018941111630553255, 'r2': -0.055467317301993946}\n",
      "{'model': {'nlayers': 4, 'nnodes': [24, 40, 20, 20, 5, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function tanh at 0x7f321d42a048>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function tanh_derivative at 0x7f321d42a0d0>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019247042378315927, 'r2': -0.07251488620499336}\n",
      "{'model': {'nlayers': 3, 'nnodes': [24, 8, 20, 25, 1], 'activations': [<function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.001830185193295546, 'r2': -0.019845452479949177}\n",
      "{'model': {'nlayers': 5, 'nnodes': [24, 5, 40, 25, 20, 25, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function tanh at 0x7f321d42a048>, <function sigmoid at 0x7f321d41fea0>, <function sigmoid at 0x7f321d41fea0>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function tanh_derivative at 0x7f321d42a0d0>, <function sigmoid_derivative at 0x7f321d41ff28>, <function sigmoid_derivative at 0x7f321d41ff28>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0020867445349025573, 'r2': -0.16280960648341125}\n",
      "{'model': {'nlayers': 5, 'nnodes': [24, 25, 40, 5, 8, 5, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function tanh at 0x7f321d42a048>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function tanh_derivative at 0x7f321d42a0d0>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019711512162220867, 'r2': -0.0983968242003963}\n",
      "{'model': {'nlayers': 4, 'nnodes': [24, 30, 20, 10, 10, 1], 'activations': [<function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.001984080945841109, 'r2': -0.10560173767148129}\n",
      "{'model': {'nlayers': 3, 'nnodes': [24, 20, 25, 10, 1], 'activations': [<function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019836658590814712, 'r2': -0.10537043630058607}\n",
      "{'model': {'nlayers': 4, 'nnodes': [24, 15, 5, 5, 5, 1], 'activations': [<function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.00199639737083545, 'r2': -0.11246489560069128}\n",
      "{'model': {'nlayers': 3, 'nnodes': [24, 25, 10, 20, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019692586939040026, 'r2': -0.09734224224503718}\n",
      "{'model': {'nlayers': 2, 'nnodes': [24, 25, 25, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0020608770461949173, 'r2': -0.14839530523007172}\n",
      "{'model': {'nlayers': 1, 'nnodes': [24, 15, 1], 'activations': [<function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0020033633167000635, 'r2': -0.11634657284202832}\n",
      "{'model': {'nlayers': 1, 'nnodes': [24, 10, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.001852268401099555, 'r2': -0.0321510154019915}\n",
      "{'model': {'nlayers': 1, 'nnodes': [24, 15, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019168256242598862, 'r2': -0.06812463747367059}\n",
      "{'model': {'nlayers': 1, 'nnodes': [24, 10, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.001995307505237743, 'r2': -0.1118575830305164}\n",
      "{'model': {'nlayers': 2, 'nnodes': [24, 10, 10, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.001963517669664971, 'r2': -0.09414313568237809}\n",
      "{'model': {'nlayers': 2, 'nnodes': [24, 20, 30, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019722012318221234, 'r2': -0.09898193090907981}\n",
      "{'model': {'nlayers': 1, 'nnodes': [24, 30, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.002067364958952202, 'r2': -0.15201060511657616}\n",
      "{'model': {'nlayers': 1, 'nnodes': [24, 20, 1], 'activations': [<function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019573691144500523, 'r2': -0.09071693810509052}\n",
      "{'model': {'nlayers': 2, 'nnodes': [24, 25, 8, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019697597595295163, 'r2': -0.09762145415290635}\n",
      "{'model': {'nlayers': 1, 'nnodes': [24, 30, 1], 'activations': [<function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0020669133743743103, 'r2': -0.1517589658399281}\n",
      "{'model': {'nlayers': 1, 'nnodes': [24, 8, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0018588248414692548, 'r2': -0.03580450135521018}\n",
      "{'model': {'nlayers': 3, 'nnodes': [24, 5, 30, 10, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.001979492451513595, 'r2': -0.1030448625034448}\n",
      "{'model': {'nlayers': 3, 'nnodes': [24, 15, 5, 30, 1], 'activations': [<function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function tanh at 0x7f321d42a048>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function tanh_derivative at 0x7f321d42a0d0>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.002115197021352683, 'r2': -0.17866436207000747}\n",
      "{'model': {'nlayers': 5, 'nnodes': [24, 40, 5, 5, 20, 10, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function sigmoid at 0x7f321d41fea0>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function sigmoid_derivative at 0x7f321d41ff28>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0018879255163116492, 'r2': -0.05202045098196528}\n",
      "{'model': {'nlayers': 1, 'nnodes': [24, 15, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.001939719991661964, 'r2': -0.08088220789219802}\n",
      "{'model': {'nlayers': 4, 'nnodes': [24, 40, 5, 10, 15, 1], 'activations': [<function relu at 0x7f321d41f400>, <function tanh at 0x7f321d42a048>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function tanh_derivative at 0x7f321d42a0d0>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0018038427327714383, 'r2': -0.0051664797338804025}\n",
      "{'model': {'nlayers': 2, 'nnodes': [24, 20, 10, 1], 'activations': [<function relu at 0x7f321d41f400>, <function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0019375458620615217, 'r2': -0.07967070416337552}\n",
      "{'model': {'nlayers': 4, 'nnodes': [24, 15, 15, 15, 8, 1], 'activations': [<function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function tanh at 0x7f321d42a048>, <function relu at 0x7f321d41f400>, <function sigmoid at 0x7f321d41fea0>, <function relu at 0x7f321d41f400>], 'derivs': [<function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function tanh_derivative at 0x7f321d42a0d0>, <function relu_derivative at 0x7f321d41fe18>, <function sigmoid_derivative at 0x7f321d41ff28>, <function relu_derivative at 0x7f321d41fe18>]}, 'mse': 0.0018568264451893377, 'r2': -0.03469092259509776}\n"
     ]
    }
   ],
   "source": [
    "model = random_search(data, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimize final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.0016641961140239996\n",
      "MSE = 0.0015084091124151662\n",
      "MSE = 0.00127252048509998\n",
      "MSE = 0.0010310213345415958\n",
      "MSE = 0.0008381873175138492\n",
      "MSE = 0.0007042955911045414\n",
      "MSE = 0.0006180388486895496\n",
      "MSE = 0.000564223313480639\n",
      "MSE = 0.000530704366050108\n",
      "MSE = 0.0005093730278224895\n",
      "MSE = 0.0004952498343780691\n",
      "MSE = 0.0004854002835832352\n",
      "MSE = 0.0004781261258760767\n",
      "MSE = 0.00047244791403138343\n",
      "MSE = 0.00046779621651777135\n",
      "MSE = 0.00046383382340152896\n",
      "MSE = 0.00046035529635739584\n",
      "MSE = 0.000457230735758699\n",
      "MSE = 0.0004543743727156162\n",
      "MSE = 0.000451726957656301\n",
      "MSE = 0.0004492457792425078\n",
      "MSE = 0.00044689890185746045\n",
      "MSE = 0.0004446617458038312\n",
      "MSE = 0.0004425149819328598\n",
      "MSE = 0.00044044317688094424\n",
      "MSE = 0.0004384338784378596\n",
      "MSE = 0.00043647696844846995\n",
      "MSE = 0.00043456418571033575\n",
      "MSE = 0.0004326887623460601\n",
      "MSE = 0.0004308451397332488\n",
      "MSE = 0.0004290287427200345\n",
      "MSE = 0.00042723579809596164\n",
      "MSE = 0.00042546318757691136\n",
      "MSE = 0.0004237083282160365\n",
      "MSE = 0.000421969074880261\n",
      "MSE = 0.0004202436406186916\n",
      "MSE = 0.0004185305316063502\n",
      "MSE = 0.0004168284939919242\n",
      "MSE = 0.0004151364704798988\n",
      "MSE = 0.00041345356487608334\n",
      "MSE = 0.0004117790131471604\n",
      "MSE = 0.0004101121598065999\n",
      "MSE = 0.0004084524386534461\n",
      "MSE = 0.0004067993570662153\n",
      "MSE = 0.0004051524831984494\n",
      "MSE = 0.0004035114355410635\n",
      "MSE = 0.00040187587441403163\n",
      "MSE = 0.00040024549502988544\n",
      "MSE = 0.00039862002183706013\n",
      "MSE = 0.00039699920390484146\n",
      "MSE = 0.0003953828111556363\n",
      "MSE = 0.00039377063128626127\n",
      "MSE = 0.0003921624672493239\n",
      "MSE = 0.00039055813518978374\n",
      "MSE = 0.00038895746275134814\n",
      "MSE = 0.00038736028768331603\n",
      "MSE = 0.00038576645669152515\n",
      "MSE = 0.00038417582448759327\n",
      "MSE = 0.00038258825299931356\n",
      "MSE = 0.00038100361071204494\n",
      "MSE = 0.0003794217721166385\n",
      "MSE = 0.0003778426172440814\n",
      "MSE = 0.00037626603127078326\n",
      "MSE = 0.0003746919041814868\n",
      "MSE = 0.00037312013047926287\n",
      "MSE = 0.00037155060893404886\n",
      "MSE = 0.0003699832423628136\n",
      "MSE = 0.0003684179374357573\n",
      "MSE = 0.0003668546045040216\n",
      "MSE = 0.00036529315744522514\n",
      "MSE = 0.0003637335135238773\n",
      "MSE = 0.00036217559326424324\n",
      "MSE = 0.00036061932033373026\n",
      "MSE = 0.0003590646214351895\n",
      "MSE = 0.0003575114262068724\n",
      "MSE = 0.000355959667128978\n",
      "MSE = 0.0003544092794359415\n",
      "MSE = 0.0003528602010337737\n",
      "MSE = 0.00035131237242186787\n",
      "MSE = 0.00034976573661880744\n",
      "MSE = 0.0003482202390917821\n",
      "MSE = 0.000346675827689287\n",
      "MSE = 0.0003451324525768226\n",
      "MSE = 0.0003435900661753756\n",
      "MSE = 0.0003420486231024699\n",
      "MSE = 0.00034050808011562133\n",
      "MSE = 0.0003389683960580449\n",
      "MSE = 0.0003374295318064776\n",
      "MSE = 0.00033589145022100696\n",
      "MSE = 0.0003343541160967823\n",
      "MSE = 0.0003328174961175287\n",
      "MSE = 0.0003312815588107566\n",
      "MSE = 0.00032974627450458764\n",
      "MSE = 0.00032821161528612735\n",
      "MSE = 0.0003266775549612878\n",
      "MSE = 0.00032514406901601\n",
      "MSE = 0.000323611134578806\n",
      "MSE = 0.0003220787303845554\n",
      "MSE = 0.0003205468367395011\n",
      "MSE = 0.00031901543548737383\n",
      "MSE = 0.000317484509976597\n",
      "MSE = 0.00031595404502850427\n",
      "MSE = 0.0003144240269065316\n",
      "MSE = 0.00031289444328631193\n",
      "MSE = 0.00031136528322664306\n",
      "MSE = 0.00030983653714126405\n",
      "MSE = 0.00030830819677140275\n",
      "MSE = 0.00030678025515904785\n",
      "MSE = 0.0003052527066208991\n",
      "MSE = 0.0003037255467229679\n",
      "MSE = 0.00030219877225576936\n",
      "MSE = 0.0003006723812100893\n",
      "MSE = 0.0002991463727532762\n",
      "MSE = 0.00029762074720603653\n",
      "MSE = 0.0002960955060196908\n",
      "MSE = 0.00029457065175387187\n",
      "MSE = 0.00029304618805463256\n",
      "MSE = 0.0002915221196329354\n",
      "MSE = 0.000289998452243503\n",
      "MSE = 0.00028847519266400953\n",
      "MSE = 0.0002869523486745856\n",
      "MSE = 0.0002854299290376239\n",
      "MSE = 0.00028390794347786524\n",
      "MSE = 0.0002823864026627528\n",
      "MSE = 0.00028086531818303313\n",
      "MSE = 0.00027934470253360144\n",
      "MSE = 0.00027782456909456924\n",
      "MSE = 0.00027630493211255457\n",
      "MSE = 0.00027478580668217895\n",
      "MSE = 0.0002732672087277645\n",
      "MSE = 0.00027174915498523125\n",
      "MSE = 0.00027023166298418216\n",
      "MSE = 0.00026871475103017536\n",
      "MSE = 0.0002671984381871827\n",
      "MSE = 0.00026568274426022923\n",
      "MSE = 0.00026416768977821306\n",
      "MSE = 0.00026265329597690895\n",
      "MSE = 0.00026113958478215344\n",
      "MSE = 0.000259626578793211\n",
      "MSE = 0.0002581143012663287\n",
      "MSE = 0.0002566027760984775\n",
      "MSE = 0.00025509202781128567\n",
      "MSE = 0.00025358208153516455\n",
      "MSE = 0.0002520729629936375\n",
      "MSE = 0.0002505646984878693\n",
      "MSE = 0.00024905731488140574\n",
      "MSE = 0.0002475508395851237\n",
      "MSE = 0.0002460453005424039\n",
      "MSE = 0.00024454072621452397\n",
      "MSE = 0.00024303714556628446\n",
      "MSE = 0.00024153458805186578\n",
      "MSE = 0.00024003308360093071\n",
      "MSE = 0.00023853266260497058\n",
      "MSE = 0.00023703335590390261\n",
      "MSE = 0.00023553519477292754\n",
      "MSE = 0.00023403821090964936\n",
      "MSE = 0.00023254243642146122\n",
      "MSE = 0.0002310479038132062\n",
      "MSE = 0.00022955464597511726\n",
      "MSE = 0.0002280626961710369\n",
      "MSE = 0.00022657208802692643\n",
      "MSE = 0.00022508285551966524\n",
      "MSE = 0.00022359503296614348\n",
      "MSE = 0.00022210865501265568\n",
      "MSE = 0.0002206237566245958\n",
      "MSE = 0.00021914037307645218\n",
      "MSE = 0.000217658539942114\n",
      "MSE = 0.00021617829308548573\n",
      "MSE = 0.00021469966865140814\n",
      "MSE = 0.00021322270305689783\n",
      "MSE = 0.00021174743298269186\n",
      "MSE = 0.0002102738953651127\n",
      "MSE = 0.0002088021273882422\n",
      "MSE = 0.00020733216647641185\n",
      "MSE = 0.00020586405028700415\n",
      "MSE = 0.00020439781670356712\n",
      "MSE = 0.00020293350382924076\n",
      "MSE = 0.0002014711499804898\n",
      "MSE = 0.00020001079368114872\n",
      "MSE = 0.0001985524736567666\n",
      "MSE = 0.00019709622882925883\n",
      "MSE = 0.0001956420983118578\n",
      "MSE = 0.00019419012140435785\n",
      "MSE = 0.0001927403375886566\n",
      "MSE = 0.0001912927865245852\n",
      "MSE = 0.00018984750804602134\n",
      "MSE = 0.00018840454215728687\n",
      "MSE = 0.00018696392902982112\n",
      "MSE = 0.00018552570899912544\n",
      "MSE = 0.00018408992256197635\n",
      "MSE = 0.00018265661037389935\n",
      "MSE = 0.00018122581324690138\n",
      "MSE = 0.00017979757214745416\n",
      "MSE = 0.00017837192819472123\n",
      "MSE = 0.0001769489226590299\n",
      "MSE = 0.000175528596960573\n",
      "MSE = 0.00017411099266834265\n",
      "MSE = 0.00017269615149928291\n",
      "MSE = 0.0001712841153176643\n",
      "MSE = 0.00016987492613466227\n",
      "MSE = 0.00016846862610814663\n",
      "MSE = 0.00016706525754266142\n",
      "MSE = 0.00016566486288960438\n",
      "MSE = 0.00016426748474758607\n",
      "MSE = 0.00016287316586297191\n",
      "MSE = 0.0001614819491305948\n",
      "MSE = 0.00016009387759463565\n",
      "MSE = 0.00015870899444966609\n",
      "MSE = 0.00015732734304184357\n",
      "MSE = 0.00015594896687025624\n",
      "MSE = 0.00015457390958840953\n",
      "MSE = 0.00015320221500585106\n",
      "MSE = 0.00015183392708992497\n",
      "MSE = 0.00015046908996765046\n",
      "MSE = 0.00014910774792772225\n",
      "MSE = 0.00014774994542262197\n",
      "MSE = 0.00014639572707084096\n",
      "MSE = 0.00014504513765920162\n",
      "MSE = 0.00014369822214528168\n",
      "MSE = 0.00014235502565992591\n",
      "MSE = 0.0001410155935098487\n",
      "MSE = 0.00013967997118031808\n",
      "MSE = 0.00013834820433791733\n",
      "MSE = 0.00013702033883338045\n",
      "MSE = 0.00013569642070449436\n",
      "MSE = 0.00013437649617906904\n",
      "MSE = 0.00013306061167796352\n",
      "MSE = 0.00013174881381817038\n",
      "MSE = 0.00013044114941595137\n",
      "MSE = 0.00012913766549002098\n",
      "MSE = 0.00012783840926477487\n",
      "MSE = 0.00012654342817355877\n",
      "MSE = 0.00012525276986197412\n",
      "MSE = 0.0001239664821912191\n",
      "MSE = 0.00012268461324145953\n",
      "MSE = 0.00012140721131522774\n",
      "MSE = 0.00012013432494084683\n",
      "MSE = 0.00011886600287587632\n",
      "MSE = 0.00011760229411057859\n",
      "MSE = 0.00011634324787140039\n",
      "MSE = 0.00011508891362447067\n",
      "MSE = 0.0001138393410791099\n",
      "MSE = 0.0001125945801913497\n",
      "MSE = 0.00011135468116746009\n",
      "MSE = 0.0001101196944674831\n",
      "MSE = 0.00010888967080877121\n",
      "MSE = 0.0001076646611695279\n",
      "MSE = 0.00010644471679234859\n",
      "MSE = 0.00010522988918776134\n",
      "MSE = 0.000104020230137767\n",
      "MSE = 0.00010281579169937298\n",
      "MSE = 0.00010161662620812615\n",
      "MSE = 0.00010042278628163612\n",
      "MSE = 9.923432482309501e-05\n",
      "MSE = 9.80512950247881e-05\n",
      "MSE = 9.687375037159475e-05\n",
      "MSE = 9.570174464448336e-05\n",
      "MSE = 9.453533192399121e-05\n",
      "MSE = 9.33745665936978e-05\n",
      "MSE = 9.221950334368454e-05\n",
      "MSE = 9.107019717398228e-05\n",
      "MSE = 8.9926703398008e-05\n",
      "MSE = 8.878907764598653e-05\n",
      "MSE = 8.765737586836073e-05\n",
      "MSE = 8.653165433918726e-05\n",
      "MSE = 8.541196965951883e-05\n",
      "MSE = 8.429837876077182e-05\n",
      "MSE = 8.319093890808142e-05\n",
      "MSE = 8.208970770364037e-05\n",
      "MSE = 8.099474309002496e-05\n",
      "MSE = 7.990610335350638e-05\n",
      "MSE = 7.882384712734694e-05\n",
      "MSE = 7.774803339508313e-05\n",
      "MSE = 7.667872149379413e-05\n",
      "MSE = 7.561597111735553e-05\n",
      "MSE = 7.455984231968018e-05\n",
      "MSE = 7.351039551794524e-05\n",
      "MSE = 7.24676914958042e-05\n",
      "MSE = 7.143179140658775e-05\n",
      "MSE = 7.040275677649028e-05\n",
      "MSE = 6.938064950774323e-05\n",
      "MSE = 6.836553188177704e-05\n",
      "MSE = 6.735746656237036e-05\n",
      "MSE = 6.635651659878701e-05\n",
      "MSE = 6.536274542890204e-05\n",
      "MSE = 6.437621688231607e-05\n",
      "MSE = 6.339699518345923e-05\n",
      "MSE = 6.2425144954684e-05\n",
      "MSE = 6.14607312193488e-05\n",
      "MSE = 6.05038194048911e-05\n",
      "MSE = 5.955447534589173e-05\n",
      "MSE = 5.861276528713046e-05\n",
      "MSE = 5.7678755886632196e-05\n",
      "MSE = 5.675251421870634e-05\n",
      "MSE = 5.583410777697772e-05\n",
      "MSE = 5.492360447741066e-05\n",
      "MSE = 5.4021072661326205e-05\n",
      "MSE = 5.312658109841287e-05\n",
      "MSE = 5.22401989897321e-05\n",
      "MSE = 5.136199597071722e-05\n",
      "MSE = 5.0492042114168615e-05\n",
      "MSE = 4.963040793324332e-05\n",
      "MSE = 4.877716438444115e-05\n",
      "MSE = 4.79323828705869e-05\n",
      "MSE = 4.709613524380897e-05\n",
      "MSE = 4.6268493808515805e-05\n",
      "MSE = 4.54495313243696e-05\n",
      "MSE = 4.463932100925774e-05\n",
      "MSE = 4.383793654226334e-05\n",
      "MSE = 4.304545206663428e-05\n",
      "MSE = 4.2261942192751466e-05\n",
      "MSE = 4.1487482001097405e-05\n",
      "MSE = 4.0722147045224115e-05\n",
      "MSE = 3.996601335472226e-05\n",
      "MSE = 3.921915743819092e-05\n",
      "MSE = 3.84816562862087e-05\n",
      "MSE = 3.775358737430713e-05\n",
      "MSE = 3.703502866594535e-05\n",
      "MSE = 3.632605861548821e-05\n",
      "MSE = 3.562675617118692e-05\n",
      "MSE = 3.49372007781629e-05\n",
      "MSE = 3.425747238139573e-05\n",
      "MSE = 3.358765142871496e-05\n",
      "MSE = 3.292781887379632e-05\n",
      "MSE = 3.2278056179163045e-05\n",
      "MSE = 3.1638445319192076e-05\n",
      "MSE = 3.100906878312603e-05\n",
      "MSE = 3.0390009578090957e-05\n",
      "MSE = 2.9781351232120453e-05\n",
      "MSE = 2.9183177797186345e-05\n",
      "MSE = 2.8595573852236136e-05\n",
      "MSE = 2.801862450623829e-05\n",
      "MSE = 2.7452415401234367e-05\n",
      "MSE = 2.6897032715399847e-05\n",
      "MSE = 2.6352563166112713e-05\n",
      "MSE = 2.5819094013030926e-05\n",
      "MSE = 2.529671306117868e-05\n",
      "MSE = 2.4785508664042023e-05\n",
      "MSE = 2.428556972667414e-05\n",
      "MSE = 2.3796985708810067e-05\n",
      "MSE = 2.33198466279922e-05\n",
      "MSE = 2.2854243062705768e-05\n",
      "MSE = 2.2400266155525485e-05\n",
      "MSE = 2.195800761627299e-05\n",
      "MSE = 2.152755972518602e-05\n",
      "MSE = 2.1109015336098918e-05\n",
      "MSE = 2.070246787963533e-05\n",
      "MSE = 2.030801136641332e-05\n",
      "MSE = 1.992574039026274e-05\n",
      "MSE = 1.9555750131455573e-05\n",
      "MSE = 1.919813635994954e-05\n",
      "MSE = 1.8852995438645085e-05\n",
      "MSE = 1.8520424326655903e-05\n",
      "MSE = 1.82005205825938e-05\n",
      "MSE = 1.7893382367867447e-05\n",
      "MSE = 1.7599108449996053e-05\n",
      "MSE = 1.7317798205937616e-05\n",
      "MSE = 1.7049551625432465e-05\n",
      "MSE = 1.6794469314362077e-05\n",
      "MSE = 1.6552652498123573e-05\n",
      "MSE = 1.632420302502038e-05\n",
      "MSE = 1.6109223369668818e-05\n",
      "MSE = 1.5907816636421435e-05\n",
      "MSE = 1.5720086562806963e-05\n",
      "MSE = 1.55461375229875e-05\n",
      "MSE = 1.5386074531232885e-05\n",
      "MSE = 1.524000324541281e-05\n",
      "MSE = 1.5108029970506737e-05\n",
      "MSE = 1.4990261662132063e-05\n",
      "MSE = 1.4886805930090821e-05\n",
      "MSE = 1.4797771041934782e-05\n",
      "MSE = 1.4723265926550175e-05\n",
      "MSE = 1.466340017776104e-05\n",
      "MSE = 1.4618284057952724e-05\n",
      "MSE = 1.4588028501714911e-05\n",
      "MSE = 1.4572745119505056e-05\n",
      "MSE = 1.4572546201332004e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4572546201332004e-05"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(nlayers=1, nnodes=[24,20,1], activations=[relu,sigmoid,relu])\n",
    "nn.gradient_descent(data,y,0.001,100,[relu_derivative,sigmoid_derivative,relu_derivative],0.00000001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
