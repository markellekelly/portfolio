{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "tensorflow.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DataPreparation.ipynb\n",
    "\n",
    "# 224 X 224 for VGG16 model,\n",
    "# in the future match our CNN\n",
    "width = 100\n",
    "scale = 0.75\n",
    "height = round(width*scale)\n",
    "data = get_datasets(width=width, height=height, which='categories')\n",
    "\n",
    "X_categories = data['X_categories']\n",
    "y_categories = data['y_categories']\n",
    "\n",
    "print(len(X_categories), 'Category Pictures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subside warnings from keras\n",
    "# (these are tensorflow warnings because of the way keras uses tensorflow,\n",
    "# not warings from our implementation)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Convolutional Neural Network for Object Categories Data\n",
    "\n",
    "( Right now only on 100 train and 100 test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prop = 0.3\n",
    "test_split = round(test_prop*len(X_categories))\n",
    "num_classes = len(set(y_categories))\n",
    "input_shape = X_categories[0].shape\n",
    "    \n",
    "# --- Shuffle Data -----------------------------------\n",
    "idxs = np.array(range(len(X_categories)))\n",
    "np.random.shuffle(idxs)\n",
    "X_categories = X_categories[idxs]\n",
    "y_categories = y_categories[idxs]\n",
    "\n",
    "# --- Data Manipulation -----------------------------------\n",
    "distinct_categories = sorted(list(set(y_categories)))\n",
    "y_categories_cat = [\n",
    "    distinct_categories.index(y) for y in y_categories\n",
    "]\n",
    "y_categories_cat = keras.utils.to_categorical(\n",
    "    y_categories_cat, num_classes\n",
    ")\n",
    "\n",
    "# --- Train Test Split -----------------------------------\n",
    "X_categories_test = X_categories[:test_split]\n",
    "y_categories_test = y_categories_cat[:test_split]\n",
    "\n",
    "X_categories_train = X_categories[test_split:]\n",
    "y_categories_train = y_categories_cat[test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(params):\n",
    "    try:\n",
    "        np.random.seed(123)\n",
    "        random.seed(123)\n",
    "        tensorflow.random.set_seed(123)\n",
    "\n",
    "        # --- Constants -----------------------------------\n",
    "        input_shape = X_categories[0].shape\n",
    "        num_classes = len(set(y_categories))\n",
    "        batch_size = params['batch_size']\n",
    "        epochs = params['epochs']\n",
    "\n",
    "        # size of window\n",
    "        kernel_size = params['kernel_size']\n",
    "\n",
    "        # factor of reduction\n",
    "        pool_size = params['pool_size']\n",
    "        strides = params['strides']\n",
    "\n",
    "        learning_rate = params['learning_rate']\n",
    "\n",
    "        if 'momentum' in params:\n",
    "            momentum = params['momentum']\n",
    "        else:\n",
    "            momentum = 0\n",
    "\n",
    "        # --- Model Architecture -----------------------------------\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(\n",
    "            64, kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            input_shape=input_shape,\n",
    "            strides = strides\n",
    "        ))\n",
    "        model.add(layers.Conv2D(\n",
    "            64, kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            strides = strides\n",
    "        ))\n",
    "        model.add(layers.MaxPooling2D(pool_size=pool_size))\n",
    "        model.add(layers.Conv2D(\n",
    "            32, kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            strides = strides\n",
    "        ))\n",
    "        model.add(layers.MaxPooling2D(pool_size=pool_size))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer=keras.optimizers.SGD(lr=learning_rate, momentum=momentum, nesterov=False),\n",
    "        #               optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "                      metrics=['categorical_accuracy'])\n",
    "\n",
    "        # --- Fit Model -----------------------------------\n",
    "        model.fit(\n",
    "            X_categories_train, y_categories_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data = (X_categories_test, y_categories_test)\n",
    "        )\n",
    "        score = model.evaluate(\n",
    "            X_categories_test, y_categories_test, \n",
    "            verbose=0\n",
    "        )\n",
    "        print(\"Score:\", score)\n",
    "        return model\n",
    "    except:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example model: 0.113 train accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = {\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.0001,\n",
    "    'epochs': 5,\n",
    "    'kernel_size': (3,3),\n",
    "    'pool_size': (2,2),\n",
    "    'strides': (1,1)\n",
    "}\n",
    "\n",
    "model1 = fit_model(params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"models/models_0.1126.pickle\", \"wb\") as f:\n",
    "    pickle.dump(model1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example model 2: 35.05% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params3 = {\n",
    "    'batch_size': 50,\n",
    "    'learning_rate': 0.0003,\n",
    "    'epochs': 14,\n",
    "    'kernel_size': (3,3),\n",
    "    'pool_size': (2,2),\n",
    "    'strides': (1,1),\n",
    "    'momentum': 0.006\n",
    "}\n",
    "\n",
    "model3 = fit_model(params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [50,100,150]\n",
    "learning_rates = [0.00007,0.0001,0.0002,0.0003]\n",
    "momentums = [0.006,0.01,0.05]\n",
    "my_lists = []\n",
    "for batch_size in batch_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        for momentum in momentums:\n",
    "            np.random.seed(123)\n",
    "            random.seed(123)\n",
    "            tensorflow.random.set_seed(123)\n",
    "            params2 = {\n",
    "                'batch_size': batch_size,\n",
    "                'learning_rate': learning_rate,\n",
    "                'epochs': 15,\n",
    "                'kernel_size': (3,3),\n",
    "                'pool_size': (2,2),\n",
    "                'strides': (1,1),\n",
    "                'momentum': momentum\n",
    "            }\n",
    "            x1,x2 = fit_model(params2)\n",
    "            my_lists.append([batch_size, learning_rate, momentum,x1,x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopped after 14 epochs\n",
    "import pickle\n",
    "with open(\"models/models_MARK.pickle\", \"wb\") as f:\n",
    "    pickle.dump(model_final, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/models_0.2383.pickle\", \"wb\") as f:\n",
    "    pickle.dump(model2, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
